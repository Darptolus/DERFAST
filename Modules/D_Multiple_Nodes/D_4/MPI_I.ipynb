{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad706c1-2a27-4d9a-8199-d873fa9a935d",
   "metadata": {},
   "source": [
    "# <center>Introduction to MPI</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43df52f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b73a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04937204",
   "metadata": {},
   "source": [
    "## MPI (Message Passing Interface)\n",
    "\n",
    "### Overview\n",
    "The Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on parallel computing architectures. MPI is widely used for parallel programming in high-performance computing (HPC) environments.\n",
    "\n",
    "MPI addresses the message-passing parallel programming model: data is **moved from the address space** of one process to that of another process through cooperative operations on each process.\n",
    "\n",
    "### MPI Standard\n",
    "The MPI standard defines the syntax and semantics of library routines that can be used to write portable message-passing programs in C, C++, and Fortran. The most current version of MPI is MPI-3.1., but The MPI standard has gone through a number of revisions, with the most recent version being MPI-4.x\n",
    "\n",
    "### MPI Implementations\n",
    "There are several implementations of the MPI standard. Two of the most widely used implementations are:\n",
    "- **MPICH**: A high-performance and widely portable implementation of MPI.\n",
    "- ** INTELMPI**: Intel specific implementation \n",
    "- **OpenMPI**: An open-source MPI implementation that is developed and maintained by a consortium of academic, research, and industry partners.\n",
    "\n",
    "OpenMPI offer MPI Build Script for Linux Clusters, \n",
    "\n",
    "\n",
    "|Implementation |language   |ScriptName | Underlying Compiler|\n",
    "|  --- |    --- |   --- |   --- |\n",
    "|Open MPI       |\tC\t    | mpicc\t    |C compiler for loaded compiler package|\n",
    "|               |   C++\t| - mpiCC <br/> - mpic++ <br/>- mpicxx\t    |C++ compiler for loaded compiler package|\n",
    "|               |   Fortran\t|   -mpif77 <br/> - mpif90\t| Fortran77 compiler for loaded compiler package <br/>Fortran90 compiler for loaded compiler package. Points to mpifort.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9740c-2564-41a8-873d-8ea09a64b1e1",
   "metadata": {},
   "source": [
    "\n",
    "## Setting Up the Environment\n",
    "To start programming with MPI in C or C++, you need to have an MPI library installed. For this tutorial, we'll use OpenMPI. Below are the steps to install and compile MPI programs using GCC and OpenMPI.\n",
    "\n",
    "### Installation of OpenMPI\n",
    "You can install OpenMPI on a Unix-based system using a package manager. For example, on Ubuntu, you can use:\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev\n",
    "```\n",
    "***Note:***: All libs and wrappers are installing, please don't try it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f2a1c-3aec-4088-b206-b18b06f7590e",
   "metadata": {},
   "source": [
    "## Compiling MPI Programs\n",
    "MPI programs are compiled using the `mpicc` or `mpiCC` compiler wrappers, which are part of the OpenMPI package. These wrappers call the underlying compiler (e.g., GCC) with the correct flags and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12386848-de9d-4d2c-82fd-b5da38a0ef39",
   "metadata": {},
   "source": [
    "\n",
    "`hello_world.c`\n",
    "```C\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int world_size;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "\n",
    "    printf(\"Hello world from rank %d out of %d processors\\n\", world_rank, world_size);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde65098-3bc1-491f-b679-6559006ab8e1",
   "metadata": {},
   "source": [
    "Let's compile the program using mpi wrapper compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dc85-f282-4452-bb2a-0ad905e2b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc Code/hello_world.c -o Code/hello_world.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff39421-c712-4aa0-95b0-a75686771958",
   "metadata": {},
   "source": [
    "which should create an executable file called `hello_world`. And now execute the program, and see in details the runtime execution using `mpirun` command or summit it to current cluster using a Slurm Job Manager with command `srun` and the parameter used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4c48a-2bf7-4664-ad86-812a5b2e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 3 ./Code/hello_world.o  # This example uses 4 processes on 2 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5502f-f786-4090-aba6-618479086184",
   "metadata": {},
   "source": [
    "Note that the execution block is enclosed in a function called `main()`, which returns the value 0 if it is completed successfully. The declaration of `main()` is mandatory in C/C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a86f4b-2a14-4838-9146-371b8f1b6b90",
   "metadata": {},
   "source": [
    "\n",
    "## MPI Basics\n",
    "\n",
    "### MPI Initialization and Finalization\n",
    "- **MPI_Init**: Initializes the MPI execution environment.\n",
    "- **MPI_Finalize**: Terminates the MPI execution environment.\n",
    "\n",
    "#### Example: Initialization and Finalization\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    printf(\"MPI environment initialized.\\n\");\n",
    "\n",
    "    MPI_Finalize();\n",
    "    printf(\"MPI environment finalized.\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Point-to-Point Communication\n",
    "#### Explanation\n",
    "- **MPI_Send**: Sends a message to another process.\n",
    "- **MPI_Recv**: Receives a message from another process.\n",
    "\n",
    "#### Example: Send and Receive\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### MPI Communicators\n",
    "#### Explanation\n",
    "- **MPI_COMM_WORLD**: Default communicator including all processes.\n",
    "- **MPI_Comm_size**: Determines the size of the group associated with a communicator.\n",
    "- **MPI_Comm_rank**: Determines the rank of the calling process in the communicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpic++ Code/mpi_send_recv.c -o code/mpi_send_recv.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07308985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 ./Code/mpi_send_recv.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58da59d",
   "metadata": {},
   "source": [
    "### MPI  Data Types\n",
    "\n",
    "To share data between nodes, is required use same data types, to cast values and manipulate side to side. MPI predefines its primitive data types:\n",
    "\n",
    "\n",
    "|C Data Types 1| C Data Types 2|\n",
    "|   ---  |  --- |\t\n",
    "|MPI_CHAR<br/>MPI_WCHAR<br/>MPI_SHORT<br/>MPI_INT<br/>MPI_LONG<br/>MPI_LONG_LONG_INT<br/>MPI_LONG_LONG<br/>MPI_SIGNED_CHAR<br/>MPI_UNSIGNED_CHAR<br/>MPI_UNSIGNED_SHORT<br/>MPI_UNSIGNED_LONG<br/>MPI_UNSIGNED<br/>MPI_FLOAT<br/>MPI_DOUBLE<br/>MPI_LONG_DOUBLE|MPI_C_COMPLEX<br/>MPI_C_FLOAT_COMPLEX<br/>MPI_C_DOUBLE_COMPLEX<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_C_BOOL<br/>MPI_LOGICAL<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_INT8_T<br/>MPI_INT16_T<br/>MPI_INT32_T<br/>MPI_INT64_T<br/>MPI_UINT8_T<br/>MPI_UINT16_T<br/>MPI_UINT32_T<br/>MPI_UINT64_T<br/>MPI_BYTE<br/>MPI_PACKED|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d3e4c",
   "metadata": {},
   "source": [
    "\n",
    "## MPI Collective Communication\n",
    "\n",
    "The Type of Collective communication on MPI are:\n",
    "\n",
    "![Collective](./img/collective_comm.gif)\n",
    "\n",
    "### Broadcast\n",
    "#### Explanation\n",
    "- **MPI_Bcast**: Broadcasts a message from the process with rank \"root\" to all other processes in the communicator.\n",
    "\n",
    "#### Example: Broadcast\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int data = 0;\n",
    "    if (world_rank == 0) {\n",
    "        data = 100;\n",
    "    }\n",
    "    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    printf(\"Process %d received data %d\\n\", world_rank, data);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0303c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|MPI Reduction Operation|\tC Data Types\t|\n",
    "|   --- |   --- |\n",
    "|MPI_MAX|\tmaximum\t|integer, float\t|\n",
    "|MPI_MIN|\tminimum\tinteger, float\t|\n",
    "|MPI_SUM|\tsum\t|integer, float\t|\n",
    "|MPI_PROD|\tproduct\t|integer, float\t|\n",
    "|MPI_LAND|\tlogical AND|\tinteger\t|\n",
    "|MPI_BAND|\tbit-wise AND|integer MPI_BYTE|\t\n",
    "|MPI_LOR|\tlogical OR|\tinteger\t|\n",
    "|MPI_BOR|\tbit-wise OR\tinteger, MPI_BYTE|\t\n",
    "|MPI_LXOR|\tlogical XOR\t|integer\t\n",
    "|MPI_BXOR|\tbit-wise XOR\t|integer, MPI_BYTE|\n",
    "|MPI_MAXLOC|\tmax value and location\t|float, double and long double|\t\n",
    "|MPI_MINLOC|\tmin value and location\t|float, double and long double|\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16284eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpic++ Code/mpi_broadcast.c -o code/mpi_broadcast.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11274034",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 3 ./Code/mpi_broadcast.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da8cd6",
   "metadata": {},
   "source": [
    "You can play with the code on mpi_broadcast.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f88306",
   "metadata": {},
   "source": [
    "In MPI, gather and scatter are collective communication operations used for data distribution and collection among processes in a parallel computing environment.\n",
    "\n",
    "### MPI Scatter:\n",
    "The MPI_Scatter operation is used to distribute distinct chunks of data from the root process to all other processes in the communicator. Each process receives a unique portion of the data. It is commonly used to divide large datasets so each process can work on its part independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d785c",
   "metadata": {},
   "source": [
    "### MPI Gather:\n",
    "The MPI_Gather operation is the reverse of scatter. It collects data from all processes and gathers them into one array in the root process. Each process contributes a portion of data, and the root process stores these in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c301083",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpic++ Code/mpi_scatter_gather.c -o code/mpi_scatter_gather.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7153251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 /Code/mpi_scatter_gather.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827b28-f0ac-4c58-a8db-b964ab2a050a",
   "metadata": {},
   "source": [
    "### Exercise: MPI Reductions with a Fixed Dataset\n",
    "Objective: In this exercise, you will learn how to use MPI reduction operations (MPI_Reduce) to combine data across multiple processes. The exercise will involve summing values across processes, where each process contributes a fixed value, and the final sum is collected and printed by the root process.\n",
    "\n",
    "#### Task:\n",
    "- Initialize MPI and obtain the rank of the process.\n",
    "- Each process will have a fixed integer value (use the process rank as the value).\n",
    "- Use MPI_Reduce to compute the sum of all the values from each process.\n",
    "- The root process (rank 0) will print the result of the sum.\n",
    "- Add code comments to explain the purpose of the various parts of the program.\n",
    "\n",
    "#### Steps:\n",
    "Set up MPI.\n",
    "- Use MPI_Reduce to sum the values of all processes.\n",
    "- Only the root process should print the final sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc Exercises/mpi_reductions.c -o Exercises/mpi_reductions.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./Exercises/mpi_reductions.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21722bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "See a possible solution to Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc Solutions/mpi_reductions.c -o Solutions/mpi_reductions.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./Solutions/mpi_reductions.o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
