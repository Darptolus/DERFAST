{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad706c1-2a27-4d9a-8199-d873fa9a935d",
   "metadata": {},
   "source": [
    "# <center>Advanced MPI</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43df52f",
   "metadata": {},
   "source": [
    "## Blocking and Non-blocking Communication in MPI\n",
    "\n",
    "### Blocking Communication\n",
    "\n",
    "Blocking communication operations in MPI, such as `MPI_Send` and `MPI_Recv`, are operations that do not return control to the user program until they are completed. This means that when a process calls `MPI_Send`, it will wait until the data has been copied out of the send buffer and is safe to be modified or reused. Similarly, when `MPI_Recv` is called, the process will wait until the data has been fully received and placed into the receive buffer.\n",
    "\n",
    "Blocking operations are straightforward to use and are suitable for many applications. However, they can lead to inefficiencies in some scenarios where processes need to wait for each other, resulting in idle time.\n",
    "\n",
    "![Blocking Communication](./img/blocking-comm.png)\n",
    "\n",
    "**Example: Blocking Send and Receive**\n",
    "\n",
    "This example demonstrates a simple blocking send and receive operation where one process sends a message to another.\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc code/blocking_send_recv.c -o code/blocking_send_recv.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program using 2,3,4 processes per node and using 2 and 3 nodes:\n",
    "!srun -N 2 ./code/blocking_send_recv.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65c297",
   "metadata": {},
   "source": [
    "### Non-blocking Communication\n",
    "\n",
    "Non-blocking communication operations in MPI, such as `MPI_Isend` and `MPI_Irecv`, allow a process to initiate a communication operation and then proceed with other computations or communications without waiting for the communication to complete. These functions return immediately, providing a `MPI_Request` object that can be used to check the status or wait for the operation to complete.\n",
    "\n",
    "Non-blocking operations are useful for overlapping communication with computation, potentially improving the performance of parallel applications by reducing idle time.\n",
    "![Non Blocking Communication](./img/non-blocking-comm.png)\n",
    "\n",
    "\n",
    "**Example: Non-blocking Send and Receive**\n",
    "\n",
    "This example demonstrates non-blocking send and receive operations where one process sends a message to another, but both processes can perform other work while waiting for the communication to complete.\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int data;\n",
    "    MPI_Request request;\n",
    "    MPI_Status status;\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        data = 123;\n",
    "        MPI_Isend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);\n",
    "        printf(\"Process 0 initiated non-blocking send of data %d\\n\", data);\n",
    "        // Perform some work while the send operation completes\n",
    "        printf(\"Process 0 is doing other work while waiting for send to complete\\n\");\n",
    "        MPI_Wait(&request, &status);  // Ensure the send operation is complete\n",
    "    } else if (world_rank == 1) {\n",
    "        MPI_Irecv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);\n",
    "        printf(\"Process 1 initiated non-blocking receive\\n\");\n",
    "        // Perform some work while the receive operation completes\n",
    "        printf(\"Process 1 is doing other work while waiting for receive to complete\\n\");\n",
    "        MPI_Wait(&request, &status);  // Ensure the receive operation is complete\n",
    "        printf(\"Process 1 received data %d\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84803c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc code/unit_1.c -o code/unit_1.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dc85-f282-4452-bb2a-0ad905e2b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise No 4\n",
    "# Save the program as `non_blocking_send_recv.c` on code folder code and compile it using `mpicc`:\n",
    "\n",
    "!mpicc ...\n",
    "\n",
    "#program using 2,3,4 processes per node and using 2 and 3 nodes:\n",
    "\n",
    "!mpirun ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff39421-c712-4aa0-95b0-a75686771958",
   "metadata": {},
   "source": [
    "\n",
    "### Comparison of Blocking and Non-blocking Communication\n",
    "\n",
    "1. **Blocking Communication**:\n",
    "   - **Pros**: Simplicity, straightforward usage.\n",
    "   - **Cons**: Potential for idle time, processes may wait for each other.\n",
    "\n",
    "2. **Non-blocking Communication**:\n",
    "   - **Pros**: Overlap communication with computation, potential performance improvements.\n",
    "   - **Cons**: More complex to use, requires careful management of `MPI_Request` objects and completion checks.\n",
    "\n",
    "Using non-blocking communication effectively requires a good understanding of the applicationâ€™s communication and computation patterns, enabling the overlap of these operations to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4c48a-2bf7-4664-ad86-812a5b2e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 ./hello_world # This example uses 4 processes on the same machine \n",
    "!!srun -N 2 -n 4 --ntasks-per-node=2 ./hello_world  # This example uses 4 processes on 2 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5502f-f786-4090-aba6-618479086184",
   "metadata": {},
   "source": [
    "Note that the execution block is enclosed in a function called `main()`, which returns the value 0 if it is completed successfully. The declaration of `main()` is mandatory in C/C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a86f4b-2a14-4838-9146-371b8f1b6b90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc code/unit_1.c -o code/unit_1.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./code/unit_1.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d3e4c",
   "metadata": {},
   "source": [
    "## MPI synchronization \n",
    "Refers to mechanisms that ensure coordination and consistency among multiple processes in a parallel computing environment. It helps control the order of execution, ensuring that processes operate in sync when necessary. Two key synchronization operations are:\n",
    "\n",
    "-**MPI_Barrier:** This is a collective operation where all processes in a communicator must reach the barrier before any of them can proceed. It ensures that all processes are synchronized at a specific point in the program.\n",
    "\n",
    "-**MPI_Wait:** This is used in conjunction with non-blocking communication, ensuring that a specific communication operation (e.g., MPI_Isend, MPI_Irecv) is completed before the process continues execution. It synchronizes a process with respect to the completion of communication tasks.\n",
    "\n",
    "These synchronization primitives help manage timing and execution in distributed environments, avoiding race conditions and ensuring correct data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827b28-f0ac-4c58-a8db-b964ab2a050a",
   "metadata": {},
   "source": [
    "### Exercise: MPI Barrier and Wait with Non-blocking Communication\n",
    "Objective: In this exercise, you will learn how to use MPI_Barrier for synchronizing all processes and MPI_Wait to handle non-blocking communication. You will modify the given code to ensure all processes reach a common point before proceeding, and ensure communication is completed before continuing execution.\n",
    "\n",
    "#### Task:\n",
    "- Initialize MPI and obtain the rank of the process.\n",
    "- Use MPI_Isend and MPI_Irecv for non-blocking communication between two processes.\n",
    "- Use MPI_Wait to ensure the communication is completed.\n",
    "- Use MPI_Barrier to synchronize all processes after the communication is done.\n",
    "- Ensure that process 1 receives the message before all processes print their rank.\n",
    "\n",
    "#### Steps:\n",
    "Set up MPI.\n",
    "- Use MPI_Isend to send data asynchronously.\n",
    "- Use MPI_Irecv to receive data asynchronously.\n",
    "- Use MPI_Wait to ensure that communication is completed before proceeding.\n",
    "- Use MPI_Barrier to ensure all processes are synchronized before printing their rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa42047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc Exercises/exercise2.c -o Exercises/exercise2.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./exercises/exercise2.o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
