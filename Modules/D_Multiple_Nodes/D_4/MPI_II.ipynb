{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad706c1-2a27-4d9a-8199-d873fa9a935d",
   "metadata": {},
   "source": [
    "# <center>Advanced MPI</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43df52f",
   "metadata": {},
   "source": [
    "## Blocking and Non-blocking Communication in MPI\n",
    "\n",
    "### Blocking Communication\n",
    "\n",
    "Blocking communication operations in MPI, such as `MPI_Send` and `MPI_Recv`, are operations that do not return control to the user program until they are completed. This means that when a process calls `MPI_Send`, it will wait until the data has been copied out of the send buffer and is safe to be modified or reused. Similarly, when `MPI_Recv` is called, the process will wait until the data has been fully received and placed into the receive buffer.\n",
    "\n",
    "Blocking operations are straightforward to use and are suitable for many applications. However, they can lead to inefficiencies in some scenarios where processes need to wait for each other, resulting in idle time.\n",
    "\n",
    "![Blocking Communication](./img/blocking-comm.png)\n",
    "\n",
    "**Example: Blocking Send and Receive**\n",
    "\n",
    "This example demonstrates a simple blocking send and receive operation where one process sends a message to another.\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc code/blocking_send_recv.c -o code/blocking_send_recv.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program using 2,3,4 processes per node and using 2 and 3 nodes:\n",
    "!srun -N 2 ./code/blocking_send_recv.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65c297",
   "metadata": {},
   "source": [
    "### Non-blocking Communication\n",
    "\n",
    "Non-blocking communication operations in MPI, such as `MPI_Isend` and `MPI_Irecv`, allow a process to initiate a communication operation and then proceed with other computations or communications without waiting for the communication to complete. These functions return immediately, providing a `MPI_Request` object that can be used to check the status or wait for the operation to complete.\n",
    "\n",
    "Non-blocking operations are useful for overlapping communication with computation, potentially improving the performance of parallel applications by reducing idle time.\n",
    "![Non Blocking Communication](./img/non-blocking-comm.png)\n",
    "\n",
    "\n",
    "**Example: Non-blocking Send and Receive**\n",
    "\n",
    "This example demonstrates non-blocking send and receive operations where one process sends a message to another, but both processes can perform other work while waiting for the communication to complete.\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int data;\n",
    "    MPI_Request request;\n",
    "    MPI_Status status;\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        data = 123;\n",
    "        MPI_Isend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);\n",
    "        printf(\"Process 0 initiated non-blocking send of data %d\\n\", data);\n",
    "        // Perform some work while the send operation completes\n",
    "        printf(\"Process 0 is doing other work while waiting for send to complete\\n\");\n",
    "        MPI_Wait(&request, &status);  // Ensure the send operation is complete\n",
    "    } else if (world_rank == 1) {\n",
    "        MPI_Irecv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);\n",
    "        printf(\"Process 1 initiated non-blocking receive\\n\");\n",
    "        // Perform some work while the receive operation completes\n",
    "        printf(\"Process 1 is doing other work while waiting for receive to complete\\n\");\n",
    "        MPI_Wait(&request, &status);  // Ensure the receive operation is complete\n",
    "        printf(\"Process 1 received data %d\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84803c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc code/unit_1.c -o code/unit_1.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dc85-f282-4452-bb2a-0ad905e2b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise No 4\n",
    "# Save the program as `non_blocking_send_recv.c` on code folder code and compile it using `mpicc`:\n",
    "\n",
    "!mpicc ...\n",
    "\n",
    "#program using 2,3,4 processes per node and using 2 and 3 nodes:\n",
    "\n",
    "!mpirun ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff39421-c712-4aa0-95b0-a75686771958",
   "metadata": {},
   "source": [
    "\n",
    "### Comparison of Blocking and Non-blocking Communication\n",
    "\n",
    "1. **Blocking Communication**:\n",
    "   - **Pros**: Simplicity, straightforward usage.\n",
    "   - **Cons**: Potential for idle time, processes may wait for each other.\n",
    "\n",
    "2. **Non-blocking Communication**:\n",
    "   - **Pros**: Overlap communication with computation, potential performance improvements.\n",
    "   - **Cons**: More complex to use, requires careful management of `MPI_Request` objects and completion checks.\n",
    "\n",
    "Using non-blocking communication effectively requires a good understanding of the applicationâ€™s communication and computation patterns, enabling the overlap of these operations to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4c48a-2bf7-4664-ad86-812a5b2e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 ./hello_world # This example uses 4 processes on the same machine \n",
    "!!srun -N 2 -n 4 --ntasks-per-node=2 ./hello_world  # This example uses 4 processes on 2 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5502f-f786-4090-aba6-618479086184",
   "metadata": {},
   "source": [
    "Note that the execution block is enclosed in a function called `main()`, which returns the value 0 if it is completed successfully. The declaration of `main()` is mandatory in C/C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a86f4b-2a14-4838-9146-371b8f1b6b90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc code/unit_1.c -o code/unit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./code/unit_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d3e4c",
   "metadata": {},
   "source": [
    "\n",
    "## Unit 2: MPI Collective Communication\n",
    "\n",
    "The Type of Collective communication on MPI are:\n",
    "\n",
    "![Collective](./img/collective_comm.gif)\n",
    "\n",
    "### Subtopic 2.1: Broadcast\n",
    "#### Explanation\n",
    "- **MPI_Bcast**: Broadcasts a message from the process with rank \"root\" to all other processes in the communicator.\n",
    "\n",
    "#### Example: Broadcast\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int data = 0;\n",
    "    if (world_rank == 0) {\n",
    "        data = 100;\n",
    "    }\n",
    "    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    printf(\"Process %d received data %d\\n\", world_rank, data);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58da59d",
   "metadata": {},
   "source": [
    "### MPI  Data Types\n",
    "\n",
    "To share data between nodes, is required use same data types, to cast values and manipulate side to side. MPI predefines its primitive data types:\n",
    "\n",
    "\n",
    "|C Data Types 1| C Data Types 2|\n",
    "|   ---  |  --- |\t\n",
    "|MPI_CHAR<br/>MPI_WCHAR<br/>MPI_SHORT<br/>MPI_INT<br/>MPI_LONG<br/>MPI_LONG_LONG_INT<br/>MPI_LONG_LONG<br/>MPI_SIGNED_CHAR<br/>MPI_UNSIGNED_CHAR<br/>MPI_UNSIGNED_SHORT<br/>MPI_UNSIGNED_LONG<br/>MPI_UNSIGNED<br/>MPI_FLOAT<br/>MPI_DOUBLE<br/>MPI_LONG_DOUBLE|MPI_C_COMPLEX<br/>MPI_C_FLOAT_COMPLEX<br/>MPI_C_DOUBLE_COMPLEX<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_C_BOOL<br/>MPI_LOGICAL<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_INT8_T<br/>MPI_INT16_T<br/>MPI_INT32_T<br/>MPI_INT64_T<br/>MPI_UINT8_T<br/>MPI_UINT16_T<br/>MPI_UINT32_T<br/>MPI_UINT64_T<br/>MPI_BYTE<br/>MPI_PACKED|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0303c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|MPI Reduction Operation|\tC Data Types\t|\n",
    "|   --- |   --- |\n",
    "|MPI_MAX|\tmaximum\t|integer, float\t|\n",
    "|MPI_MIN|\tminimum\tinteger, float\t|\n",
    "|MPI_SUM|\tsum\t|integer, float\t|\n",
    "|MPI_PROD|\tproduct\t|integer, float\t|\n",
    "|MPI_LAND|\tlogical AND|\tinteger\t|\n",
    "|MPI_BAND|\tbit-wise AND|integer MPI_BYTE|\t\n",
    "|MPI_LOR|\tlogical OR|\tinteger\t|\n",
    "|MPI_BOR|\tbit-wise OR\tinteger, MPI_BYTE|\t\n",
    "|MPI_LXOR|\tlogical XOR\t|integer\t\n",
    "|MPI_BXOR|\tbit-wise XOR\t|integer, MPI_BYTE|\n",
    "|MPI_MAXLOC|\tmax value and location\t|float, double and long double|\t\n",
    "|MPI_MINLOC|\tmin value and location\t|float, double and long double|\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827b28-f0ac-4c58-a8db-b964ab2a050a",
   "metadata": {},
   "source": [
    "### Exercise No 2. \n",
    "\n",
    "Please compile  with support for MPI and Submit it to Job Manager to compile and run the example of `distributed_sum.c`, use several configuration to runtime: Example 2 nodes with 2 Process per node,  with 4 process per node, 3, 4 Nodes too.  Please comment the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa42047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun mpicc exercises/unit_1.c -o exercises/unit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./exercises/unit_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
